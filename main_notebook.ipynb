{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from conversation import create_coder, create_reviewer, create_refiner, start_conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho de RL - Qual o Melhor Prompt para Iterar sobre a Gera√ß√£o de C√≥digo?\n",
    "Nosso trabalho de RL √© sobre descobrir quais os melhores prompts para iterar sobre a gera√ß√£o de \n",
    "c√≥digo de LLMs.\n",
    "\n",
    "## Como funciona\n",
    "Criamos uma conversa com 3 participantes: Coder, Reviewer e Refiner.\n",
    "Cada participante √© respons√°vel por enviar um prompt para o LLM, e inputar os resultados do LLM no\n",
    "ambiente (aqui chamado de 'conversa').\n",
    "\n",
    "### Coder\n",
    "O Coder √© respons√°vel por escrever o c√≥digo.  \n",
    "Ele envia o prompt inicial, descrevendo o problema a ser resolvido. Definimos que todos os nossos\n",
    "problemas ser√£o de limpeza de uma base de dados csv.  \n",
    "O Coder s√≥ participa da conversa 1 vez (no in√≠cio) e, por isso, n√£o o definimos como um agente RL.\n",
    "Ao inv√©s disso, ele √© programado para iterar por todos os prompts n vezes, e avaliamos os resultados\n",
    "das conversas com cada prompt inicial posteriormente.\n",
    "\n",
    "### Reviewer\n",
    "O Reviewer √© respons√°vel por avaliar o c√≥digo gerado pelo LLM.  \n",
    "Ele envia um prompt solicitando a avalia√ß√£o do c√≥digo gerado pelo LLM. Ele pode essa avalia√ß√£o\n",
    "sempre ap√≥s a gera√ß√£o de um c√≥digo que n√£o tem nota superior √† nota terminal.  \n",
    "O Reviewer √© um agente RL, e seu objetivo √© maximizar a nota do c√≥digo gerado pelo LLM.\n",
    "\n",
    "### Refiner\n",
    "O Refiner √© respons√°vel por refinar o c√≥digo gerado pelo LLM.\n",
    "Ele envia um prompt solicitando a melhoria do c√≥digo gerado pelo LLM. Ele pode essa avalia√ß√£o\n",
    "sempre ap√≥s uma revis√£o do Reviewer.  \n",
    "O Refiner √© um agente RL, e seu objetivo √© maximizar a nota do c√≥digo gerado pelo LLM.\n",
    "\n",
    "### Prompt\n",
    "Para cada participante, geramos prompts que iam de 1 a $n$ nas **escalas** das seguintes **propriedades**:\n",
    "- Clareza;\n",
    "- Comprimento;\n",
    "- Especificidade, e\n",
    "- Complexidade.\n",
    "\n",
    "Isso totalizou at√© 20 prompts diferentes para cada participante. Para diminuir o espa√ßo de a√ß√µes,\n",
    "optamos por usar uma estrat√©gia mais simples:\n",
    "\n",
    "- Para cada prompt (**comprimento da escala** x **n√∫mero de propriedades**) do **Coder**;\n",
    "    - Para cada **propriedade** do **Reviewer**;\n",
    "        - Para cada **propriedade** do **Refiner**;\n",
    "            - Geramos $m$ conversas onde:\n",
    "                1. O Coder envia o prompt e adiciona o c√≥digo inicial;\n",
    "                2. O c√≥digo √© avaliado (se a nota n√£o for terminal, prossegue);\n",
    "                3. O Reviewer escolhe um dos $n$ prompts da **propriedade** e adiciona a revis√£o;\n",
    "                4. O Refiner escolhe um dos $n$ prompts da **propriedade** e adiciona a melhoria.\n",
    "                5. Se o comprimento da conversa n√£o for terminal, volta ao passo 2.\n",
    "\n",
    "### Avalia√ß√£o do C√≥digo\n",
    "O c√≥digo √© avaliado por um LLM usando a bibliteca `instructor`. Pedimos que o c√≥digo receba uma nota\n",
    "de 0 a 100 para a sua corretude e legibilidade, bem como uma curta explica√ß√£o do porqu√™ da nota \n",
    "(esse coment√°rio √© adicionado posteriormente √† conversa).  \n",
    "Se a nota m√©dia for superior a 95, a conversa √© terminada pois consideramos que o c√≥digo √© bom o\n",
    "suficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List with JSON files name\n",
    "json_files_coder = [\n",
    "    \"json_files/prompts_clarity_coder.json\",\n",
    "    \"json_files/prompts_size_coder.json\",\n",
    "    \"json_files/prompts_specificity_coder.json\",\n",
    "    \"json_files/prompts_complexity_coder.json\"\n",
    "]\n",
    "\n",
    "json_files_reviewer = [\n",
    "    \"json_files/prompts_clarity_reviewer.json\",\n",
    "    \"json_files/prompts_size_reviewer.json\",\n",
    "    \"json_files/prompts_specificity_reviewer.json\",\n",
    "    \"json_files/prompts_complexity_reviewer.json\"\n",
    "]\n",
    "\n",
    "json_files_refiner = [\n",
    "    \"json_files/prompts_prop1_refiner.json\",\n",
    "    \"json_files/prompts_prop2_refiner.json\",\n",
    "    \"json_files/prompts_prop3_refiner.json\",\n",
    "    \"json_files/prompts_prop4_refiner.json\"\n",
    "]\n",
    "\n",
    "prompts_coder = []\n",
    "for file_name in json_files_coder:\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "        for i, item in enumerate(data):\n",
    "            item[\"index\"] = i\n",
    "        prompts_coder += data\n",
    "\n",
    "reviewer_properties = {}    \n",
    "for file_name in json_files_reviewer:\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "        for item in data:\n",
    "            if item[\"propriedade\"] not in reviewer_properties:\n",
    "                reviewer_properties[item[\"propriedade\"]] = []\n",
    "            reviewer_properties[item[\"propriedade\"]].append(item['prompt'])\n",
    "\n",
    "refiner_properties = {}\n",
    "for file_name in json_files_coder:\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "        for item in data:\n",
    "            if item[\"propriedade\"] not in refiner_properties:\n",
    "                refiner_properties[item[\"propriedade\"]] = []\n",
    "            refiner_properties[item[\"propriedade\"]].append(item['prompt'])                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMAX_TURNS = 1\\nTOT_CONVERSATIONS = 2\\ncoder = create_coder(prompts_coder)\\n\\nfor coder_prompt_dict in prompts_coder:\\n    for rev_prop, rev_prompts in reviewer_properties.items():\\n        for ref_prop, ref_prompts in refiner_properties.items():\\n            reviewer = create_reviewer(rev_prompts)\\n            refiner = create_refiner(ref_prompts)\\n            for _ in tqdm.tqdm(range(TOT_CONVERSATIONS), desc=\"Conversations\", position=0):\\n                try:\\n                    start_conversation(\\n                        coder, \\n                        coder_prompt_dict, \\n                        reviewer, \\n                        refiner, \\n                        MAX_TURNS\\n                    )\\n                except Exception as e:\\n                    print(f\"Conversation Skipped: {e}\")\\n            # Saving the models\\n            with open(f\"models/reviewer_{coder_prompt_dict}_{rev_prop}_{ref_prop}.pkl\", \"wb\") as file:\\n                pickle.dump(reviewer, file)\\n            with open(f\"models/refiner_{coder_prompt_dict}_{rev_prop}_{ref_prop}.pkl\", \"wb\") as file:\\n                pickle.dump(refiner, file)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "MAX_TURNS = 1\n",
    "TOT_CONVERSATIONS = 2\n",
    "coder = create_coder(prompts_coder)\n",
    "\n",
    "for coder_prompt_dict in prompts_coder:\n",
    "    for rev_prop, rev_prompts in reviewer_properties.items():\n",
    "        for ref_prop, ref_prompts in refiner_properties.items():\n",
    "            reviewer = create_reviewer(rev_prompts)\n",
    "            refiner = create_refiner(ref_prompts)\n",
    "            for _ in tqdm.tqdm(range(TOT_CONVERSATIONS), desc=\"Conversations\", position=0):\n",
    "                try:\n",
    "                    start_conversation(\n",
    "                        coder, \n",
    "                        coder_prompt_dict, \n",
    "                        reviewer, \n",
    "                        refiner, \n",
    "                        MAX_TURNS\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Conversation Skipped: {e}\")\n",
    "            # Saving the models\n",
    "            with open(f\"models/reviewer_{coder_prompt_dict}_{rev_prop}_{ref_prop}.pkl\", \"wb\") as file:\n",
    "                pickle.dump(reviewer, file)\n",
    "            with open(f\"models/refiner_{coder_prompt_dict}_{rev_prop}_{ref_prop}.pkl\", \"wb\") as file:\n",
    "                pickle.dump(refiner, file)\n",
    "'''                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Conversations:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**User**: I gotta clean up this messed-up CSV file...  Python, please! ü•∫\n",
       "\n",
       "Here is the `imdb_sample_10.csv` file content:\n",
       "\n",
       "Series_Title,Released_Year,Runtime,Genre,IMDB_Rating,Meta_score,Director,No_of_Votes\n",
       "The Shawshank Redemption,1994,,Drama,nan,nan,,nan\n",
       ",1972,,\"Crime, Drama\",nan,nan,,nan\n",
       "The Dark Knight,,,\"Action, Crime, Drama\",,,Christopher Nolan,\n",
       "The Godfather: Part II,nan,,\"Crime, Drama\",9.0,90.0,Francis Ford Coppola,nan\n",
       ",1957,,\"Crime, Drama\",9.0,96.0,Sidney Lumet,nan\n",
       "The Lord of the Rings: The Return of the King,2003,201 min,,8.9,94.0,Peter Jackson,nan\n",
       ",,154 min,\"Crime, Drama\",,,,\n",
       "Schindler's List,,195 min,,,,,\n",
       "Inception,,148 min,\"Action, Adventure, Sci-Fi\",,,Christopher Nolan,\n",
       "Fight Club,nan,139 min,Drama,8.8,66.0,David Fincher,nan\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**assistant**: Sent by Coder: \n",
       "\n",
       "You got it! Let's clean up that CSV file using Python. Here's a breakdown of how to tackle this and some key code snippets:\n",
       "\n",
       "**1. Importing Libraries**\n",
       "   ```python\n",
       "   import pandas as pd \n",
       "   ```\n",
       "   We use the `pandas` library for its powerful data manipulation capabilities.  \n",
       "\n",
       "**2. Loading the CSV**\n",
       "   ```python\n",
       "   data = pd.read_csv('imdb_sample_10.csv')\n",
       "   print(data.head()) # Look at the first few rows to inspect \n",
       "   ```\n",
       "\n",
       "**3. Cleaning and Formatting**\n",
       "\n",
       "* **Handling Missing Values (NaNs):**  We want to replace these with more sensible values, like empty strings.\n",
       "    ```python\n",
       "    data['IMDB_Rating'] = data['IMDB_Rating'].fillna('')  \n",
       "    data['Meta_score'] = data['Meta_score'].fillna('') \n",
       "\n",
       "    print(data) # See the updated DataFrame\n",
       "    ```\n",
       "* **Dealing with Unnecessary Data**  We'll drop the unwanted column like \"No_of_Votes\".\n",
       "    ```python \n",
       "    data = data.drop(['No_of_Votes'], axis=1) \n",
       "   ```\n",
       "    \n",
       "* **Formatting and Cleaning Titles:**  Convert title entries into something more standardized\n",
       " ```python\n",
       "    data['Series_Title'] = data['Series_Title'].str.strip()\n",
       "     \n",
       "    print(data.head())\n",
       "    ```\n",
       "\n",
       "**4. Saving the Cleaned Data (Optional)**\n",
       "   ```python\n",
       "   data.to_csv('cleaned_imdb_sample.csv', index=False) \n",
       "   ```\n",
       "\n",
       "\n",
       "\n",
       "**Explanation:**\n",
       "\n",
       "* **`pd.read_csv()`:**  This reads your CSV file and creates a pandas DataFrame ‚Äì a structured table-like data object.\n",
       "* **`.head()`:** Displays the first few rows of the DataFrame to get an idea of what you're working with. \n",
       "* **`fillna('')`:**  Replaces `NaN` values (Not a Number) in columns like \"IMDB_Rating\" and \"Meta_score\" with empty strings, which is often better than leaving them as `NaN`.\n",
       "\n",
       "**Important Notes**\n",
       "\n",
       "\n",
       "* **Error Handling:** If you encounter unexpected errors or missing data, it's best to include error handling.\n",
       "* **Data Validation:** Before applying any transformations or cleaning, it's good practice to check your data for consistency and make sure it meets your needs. \n",
       "\n",
       "\n",
       "\n",
       "Let me know if you have more specific requirements about how you want to clean your dataset!  "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**assistant**: Sent by Code Evaluator: \n",
       "\n",
       "**Correctness:** 90\n",
       "\n",
       "**Grade Explanation:** The code leverages the pandas library for its efficient data manipulation capabilities and uses clear logic to handle missing values, remove irrelevant columns and format titles. This demonstrates a good understanding of cleaning data and setting up data structures.\n",
       "\n",
       "**Readability:** 85\n",
       "\n",
       "**Grade Explanation:** The explanation is generally well-structured with step-by-step explanations for each code section but can benefit from more detailed context around the specific libraries being used when referring to them."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**assistant**: Sent by Reviewer: \n",
       "\n",
       "You got it! Let's take a deeper dive into that Python program and see if we can sharpen its performance. \n",
       "\n",
       "**Reviewing the Code:**\n",
       "\n",
       "The code you provided shows an attempt to clean up a CSV file, but there are some issues:\n",
       "\n",
       "\n",
       "* **Inconsistent Formatting:**  It seems like data is being cleaned in different ways (like converting titles).\n",
       "* **Missing Context:** The code jumps straight into actions without providing clear instructions about what it's trying to do or where the data is coming from. \n",
       "\n",
       "**Here‚Äôs an improved approach, focusing on clarity and better organization**:\n",
       "\n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "\n",
       "# Load the CSV file into a Pandas DataFrame\n",
       "data = pd.read_csv('imdb_sample_10.csv')  # Replace with your file path\n",
       "\n",
       "print(data.head()) # Look at first few rows\n",
       "\n",
       "# 1. Data Cleaning: Addressing Missing Values & Column Formatting\n",
       "\n",
       "# Replacing NaN values with empty strings (good practice)\n",
       "data['IMDB_Rating'] = data['IMDB_Rating'].fillna('')\n",
       "data['Meta_score'] = data['Meta_score'].fillna('') \n",
       "print(data)  \n",
       "\n",
       "# 2. Data Manipulation: Removing Unnecessary Columns \n",
       "data = data.drop(['No_of_Votes'], axis=1) # Removing unnecessary column \n",
       "\n",
       "# 3. Formatting:  Clean Titles and Other Considerations \n",
       "# ... (Add specific formatting here based on your requirements)\n",
       "\n",
       "\n",
       "# Save the cleaned DataFrame to a new CSV file\n",
       "data.to_csv('cleaned_imdb_sample.csv', index=False)  \n",
       "```\n",
       "\n",
       "\n",
       "\n",
       "**Explanation:**\n",
       "\n",
       "* **Clarity First:** The code is now structured and explained. \n",
       "    * `print(data.head())` shows us the first few rows, which helps you inspect your data before and after cleaning. \n",
       "\n",
       "\n",
       "* **Comments:** Adding comments to explain what's going on will make it easier for others (and for you) to understand.\n",
       "\n",
       "**Tips for Reviewers**\n",
       "   \n",
       "  You can always ask me:\n",
       "    * **\"What does this code do?\"** \n",
       "    * **\"How is the code using Pandas?\"**\n",
       "    * **\"Could you explain the logic behind certain steps?\"** \n",
       "\n",
       "\n",
       "I'm here to help you polish your code and make it even more robust. Let's work together!\n",
       "\n",
       "\n",
       "\n",
       "Let me know what parts you want to tackle next. I can help with formatting, addressing edge cases, or suggesting additional improvements!  \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**assistant**: Sent by Code Refiner: \n",
       "\n",
       "You got it! Let's tackle that messy CSV file using Python. I'll guide you through the process and clear things up along the way! Here's a breakdown of how we can do this. \n",
       "\n",
       "**1. Loading Your Data (Pandas to the Rescue)**\n",
       "   ```python\n",
       "   import pandas as pd \n",
       "\n",
       "   data = pd.read_csv('your_file.csv') # Replace 'your_file.csv' with your actual file path!\n",
       "   print(data.head())  # Quick look at first few rows\n",
       "   ```\n",
       "\n",
       "**Explanation:**\n",
       "* **`import pandas as pd`**: We bring in the powerful pandas library, which is our tool for working with data. The `pd` short form makes things easier to use. \n",
       "* **`data = pd.read_csv('your_file.csv')`**:  This tells pandas where your CSV file is located and reads it into a DataFrame. This is our structured table! \n",
       "\n",
       "**2. Cleaning Up Your Data (The Power of Pandas)**\n",
       "   ```python\n",
       "   # Dealing with missing data: \n",
       "   data['Column_Name'].fillna(0) # Replace 'Column_Name' with the actual column name\n",
       "   print(data.head())  \n",
       "   ```\n",
       "    \n",
       "**Explanation:**\n",
       "\n",
       "* **`data['Column_Name'] = data['Column_Name'].fillna(0)`**: We fill `NaN` (Not a Number) values in specific columns like 'MovieTitle' with 0 to make it easier! You'll likely need to choose the right column name based on your data. \n",
       "   \n",
       "\n",
       "**3. Formatting and Cleaning Up:**\n",
       "   ```python\n",
       "   data['Column_Name'] = data['Column_Name'].str.title() # Make title case \n",
       "   print(data.head())  \n",
       "   ``` \n",
       "    \n",
       "   **Explanation:**\n",
       "* **`data['Column_Name'] = ...`**: We use `str.title()` to capitalize the first letter of each word in 'Column_name' (and 'MovieTitle') to make them easier to read!\n",
       "\n",
       "**4. Saving Your Cleaned Data:**\n",
       "   ```python\n",
       "   data.to_csv('cleaned_data.csv', index=False)  # Make sure you replace 'cleaned_data.csv' with your desired file name \n",
       "   ``` \n",
       "\n",
       "\n",
       "**Explanation:**\n",
       "\n",
       "* **`data.to_csv('cleaned_data.csv', index=False)`**: Saves our clean DataFrame as a new CSV file! \n",
       "\n",
       "\n",
       "**Important Notes:**\n",
       "\n",
       "* **File Path:** Make sure you replace `\"your_file.csv\"` with the correct path to your actual CSV file on your computer.\n",
       "* **Debugging:** If you run into any issues, try running this code and see what happens! We can use Python's error-handling capabilities if needed.  \n",
       "\n",
       "\n",
       "Let me know what questions you have or what parts you want help with specifically! \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Conversations:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [01:18<01:18, 78.32s/it]\n"
     ]
    },
    {
     "ename": "InstructorRetryException",
     "evalue": "4 validation errors for CodeEvaluation\ncorrectness_grade\n  Field required [type=missing, input_value={'properties': {'correctn...planation_readability']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\nexplanation_correctness\n  Field required [type=missing, input_value={'properties': {'correctn...planation_readability']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\nreadability_grade\n  Field required [type=missing, input_value={'properties': {'correctn...planation_readability']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\nexplanation_readability\n  Field required [type=missing, input_value={'properties': {'correctn...planation_readability']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\lagua\\OneDrive\\Documentos\\git-folders\\RL-code-generation-with-multiple-agents\\.venv\\Lib\\site-packages\\instructor\\retry.py:225\u001b[0m, in \u001b[0;36mretry_sync\u001b[1;34m(func, response_model, args, kwargs, context, max_retries, strict, mode)\u001b[0m\n\u001b[0;32m    222\u001b[0m                     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m merge_consecutive_messages(\n\u001b[0;32m    223\u001b[0m                         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    224\u001b[0m                     )\n\u001b[1;32m--> 225\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\lagua\\OneDrive\\Documentos\\git-folders\\RL-code-generation-with-multiple-agents\\.venv\\Lib\\site-packages\\instructor\\retry.py:195\u001b[0m, in \u001b[0;36mretry_sync\u001b[1;34m(func, response_model, args, kwargs, context, max_retries, strict, mode)\u001b[0m\n\u001b[0;32m    194\u001b[0m     response \u001b[38;5;241m=\u001b[39m update_total_usage(response, total_usage)\n\u001b[1;32m--> 195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprocess_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ValidationError, JSONDecodeError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\lagua\\OneDrive\\Documentos\\git-folders\\RL-code-generation-with-multiple-agents\\.venv\\Lib\\site-packages\\instructor\\process_response.py:153\u001b[0m, in \u001b[0;36mprocess_response\u001b[1;34m(response, response_model, stream, validation_context, strict, mode)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m--> 153\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mresponse_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# ? This really hints at the fact that we need a better way of\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# ? attaching usage data and the raw response to the model we return.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lagua\\OneDrive\\Documentos\\git-folders\\RL-code-generation-with-multiple-agents\\.venv\\Lib\\site-packages\\instructor\\function_calls.py:161\u001b[0m, in \u001b[0;36mOpenAISchema.from_response\u001b[1;34m(cls, completion, validation_context, strict, mode)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    155\u001b[0m     Mode\u001b[38;5;241m.\u001b[39mJSON,\n\u001b[0;32m    156\u001b[0m     Mode\u001b[38;5;241m.\u001b[39mJSON_SCHEMA,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m     Mode\u001b[38;5;241m.\u001b[39mCEREBRAS_JSON,\n\u001b[0;32m    160\u001b[0m }:\n\u001b[1;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid patch mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lagua\\OneDrive\\Documentos\\git-folders\\RL-code-generation-with-multiple-agents\\.venv\\Lib\\site-packages\\instructor\\function_calls.py:358\u001b[0m, in \u001b[0;36mOpenAISchema.parse_json\u001b[1;34m(cls, completion, validation_context, strict)\u001b[0m\n\u001b[0;32m    356\u001b[0m message \u001b[38;5;241m=\u001b[39m extract_json_from_codeblock(message)\n\u001b[1;32m--> 358\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_validate_json\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lagua\\OneDrive\\Documentos\\git-folders\\RL-code-generation-with-multiple-agents\\.venv\\Lib\\site-packages\\pydantic\\main.py:625\u001b[0m, in \u001b[0;36mBaseModel.model_validate_json\u001b[1;34m(cls, json_data, strict, context)\u001b[0m\n\u001b[0;32m    624\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 625\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValidationError\u001b[0m: 4 validation errors for CodeEvaluation\ncorrectness_grade\n  Field required [type=missing, input_value={'properties': {'correctn...planation_readability']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\nexplanation_correctness\n  Field required [type=missing, input_value={'properties': {'correctn...planation_readability']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\nreadability_grade\n  Field required [type=missing, input_value={'properties': {'correctn...planation_readability']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\nexplanation_readability\n  Field required [type=missing, input_value={'properties': {'correctn...planation_readability']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRetryError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\lagua\\OneDrive\\Documentos\\git-folders\\RL-code-generation-with-multiple-agents\\.venv\\Lib\\site-packages\\instructor\\retry.py:189\u001b[0m, in \u001b[0;36mretry_sync\u001b[1;34m(func, response_model, args, kwargs, context, max_retries, strict, mode)\u001b[0m\n\u001b[0;32m    188\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattempt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mattempt\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lagua\\OneDrive\\Documentos\\git-folders\\RL-code-generation-with-multiple-agents\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:443\u001b[0m, in \u001b[0;36mBaseRetrying.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n",
      "File \u001b[1;32mc:\\Users\\lagua\\OneDrive\\Documentos\\git-folders\\RL-code-generation-with-multiple-agents\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:376\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[1;32m--> 376\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\lagua\\OneDrive\\Documentos\\git-folders\\RL-code-generation-with-multiple-agents\\.venv\\Lib\\site-packages\\tenacity\\__init__.py:419\u001b[0m, in \u001b[0;36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[1;34m(rs)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retry_exc\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[1;32m--> 419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n",
      "\u001b[1;31mRetryError\u001b[0m: RetryError[<Future at 0x22629617f20 state=finished raised ValidationError>]",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mInstructorRetryException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 20\u001b[0m\n\u001b[0;32m     16\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m CodeEvaluator(environment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluate the code quality\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode Evaluator\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(TOT_CONVERSATIONS), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversations\u001b[39m\u001b[38;5;124m\"\u001b[39m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m#try:\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m         environment \u001b[38;5;241m=\u001b[39m \u001b[43mstart_conversation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcoder_prompt_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreviewer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrefiner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[43mMAX_TURNS\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m environment\u001b[38;5;241m.\u001b[39mmessages:\n\u001b[0;32m     28\u001b[0m             display(Markdown(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m**: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\lagua\\OneDrive\\Documentos\\git-folders\\RL-code-generation-with-multiple-agents\\conversation.py:118\u001b[0m, in \u001b[0;36mstart_conversation\u001b[1;34m(coder, coder_prompt_dict, reviewer, refiner, max_turns)\u001b[0m\n\u001b[0;32m    115\u001b[0m last_grade \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m turn \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(max_turns), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConv. turns\u001b[39m\u001b[38;5;124m\"\u001b[39m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# Evaluates the code and status of the CSV\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m     grade \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# Check if the score is enough to close\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_terminate_grade(grade):\n",
      "File \u001b[1;32mc:\\Users\\lagua\\OneDrive\\Documentos\\git-folders\\RL-code-generation-with-multiple-agents\\rl\\code_evaluator.py:51\u001b[0m, in \u001b[0;36mCodeEvaluator.evaluate_code\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m last_code_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mget_last_message(owner\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoder\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m reduced_environment \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m last_code_msg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]}]\n\u001b[1;32m---> 51\u001b[0m rewards, message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_instructor_for_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduced_environment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Adds feedback to the environment\u001b[39;00m\n\u001b[0;32m     54\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmark_name_on_message(message)\n",
      "File \u001b[1;32mc:\\Users\\lagua\\OneDrive\\Documentos\\git-folders\\RL-code-generation-with-multiple-agents\\rl\\code_evaluator.py:84\u001b[0m, in \u001b[0;36mCodeEvaluator.call_instructor_for_evaluation\u001b[1;34m(self, messages)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_instructor_for_evaluation\u001b[39m(\u001b[38;5;28mself\u001b[39m, messages: \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m---> 84\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCodeEvaluation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_mean_grade(), {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m: response\u001b[38;5;241m.\u001b[39mget_answer()}\n",
      "File \u001b[1;32mc:\\Users\\lagua\\OneDrive\\Documentos\\git-folders\\RL-code-generation-with-multiple-agents\\.venv\\Lib\\site-packages\\instructor\\client.py:119\u001b[0m, in \u001b[0;36mInstructor.create\u001b[1;34m(self, response_model, messages, max_retries, validation_context, context, strict, **kwargs)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    109\u001b[0m     response_model: \u001b[38;5;28mtype\u001b[39m[T] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    116\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T \u001b[38;5;241m|\u001b[39m Any \u001b[38;5;241m|\u001b[39m Awaitable[T] \u001b[38;5;241m|\u001b[39m Awaitable[Any]:\n\u001b[0;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_kwargs(kwargs)\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lagua\\OneDrive\\Documentos\\git-folders\\RL-code-generation-with-multiple-agents\\.venv\\Lib\\site-packages\\instructor\\patch.py:291\u001b[0m, in \u001b[0;36mpatch.<locals>.new_create_sync\u001b[1;34m(response_model, validation_context, context, max_retries, strict, *args, **kwargs)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontents\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m new_kwargs:\n\u001b[0;32m    289\u001b[0m     new_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontents\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m handle_templating(new_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontents\u001b[39m\u001b[38;5;124m\"\u001b[39m], context)\n\u001b[1;32m--> 291\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mretry_sync\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\lagua\\OneDrive\\Documentos\\git-folders\\RL-code-generation-with-multiple-agents\\.venv\\Lib\\site-packages\\instructor\\retry.py:227\u001b[0m, in \u001b[0;36mretry_sync\u001b[1;34m(func, response_model, args, kwargs, context, max_retries, strict, mode)\u001b[0m\n\u001b[0;32m    225\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 227\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InstructorRetryException(\n\u001b[0;32m    228\u001b[0m         e\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39m_exception,\n\u001b[0;32m    229\u001b[0m         last_completion\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[0;32m    230\u001b[0m         n_attempts\u001b[38;5;241m=\u001b[39mattempt\u001b[38;5;241m.\u001b[39mretry_state\u001b[38;5;241m.\u001b[39mattempt_number,\n\u001b[0;32m    231\u001b[0m         messages\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    232\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontents\u001b[39m\u001b[38;5;124m\"\u001b[39m, kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m, []))\n\u001b[0;32m    233\u001b[0m         ),\n\u001b[0;32m    234\u001b[0m         total_usage\u001b[38;5;241m=\u001b[39mtotal_usage,\n\u001b[0;32m    235\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mInstructorRetryException\u001b[0m: 4 validation errors for CodeEvaluation\ncorrectness_grade\n  Field required [type=missing, input_value={'properties': {'correctn...planation_readability']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\nexplanation_correctness\n  Field required [type=missing, input_value={'properties': {'correctn...planation_readability']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\nreadability_grade\n  Field required [type=missing, input_value={'properties': {'correctn...planation_readability']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing\nexplanation_readability\n  Field required [type=missing, input_value={'properties': {'correctn...planation_readability']}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.9/v/missing"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from rl.code_evaluator import CodeEvaluator\n",
    "\n",
    "\n",
    "MAX_TURNS = 1\n",
    "TOT_CONVERSATIONS = 2\n",
    "coder = create_coder(prompts_coder)\n",
    "\n",
    "for i, coder_prompt_dict in enumerate(prompts_coder):\n",
    "    for j, (rev_prop, rev_prompts) in enumerate(reviewer_properties.items()):\n",
    "        for k, (ref_prop, ref_prompts) in enumerate(refiner_properties.items()):\n",
    "            reviewer = create_reviewer(rev_prompts)\n",
    "            refiner = create_refiner(ref_prompts)\n",
    "            evaluator = CodeEvaluator(environment=None, prompt=\"Evaluate the code quality\", name=\"Code Evaluator\")\n",
    "            \n",
    "            for _ in tqdm.tqdm(range(TOT_CONVERSATIONS), desc=\"Conversations\", position=0):\n",
    "                #try:\n",
    "                    environment = start_conversation(\n",
    "                        coder, \n",
    "                        coder_prompt_dict, \n",
    "                        reviewer, \n",
    "                        refiner, \n",
    "                        MAX_TURNS\n",
    "                    )\n",
    "                    for message in environment.messages:\n",
    "                        display(Markdown(f\"**{message['role']}**: {message['content']}\"))\n",
    "                \n",
    "                #except Exception as e:\n",
    "                #    print(f\"Conversation Skipped: {e}\")\n",
    "\n",
    "            # Salva os modelos\n",
    "            with open(f\"models/reviewer_{i}_{j}_{k}.pkl\", \"wb\") as file:\n",
    "                pickle.dump(reviewer, file)\n",
    "            with open(f\"models/refiner_{i}_{j}_{k}.pkl\", \"wb\") as file:\n",
    "                pickle.dump(refiner, file)\n",
    "                              \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
